{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Connectedness\n",
    "Author: Ioannis Vougias\n",
    "ioannisvougias@hotmail.com\n",
    "\n",
    "This notebook contains several visual replications of figures in Python based on the the following papers.\n",
    "\n",
    "* Chetty, R., Jackson, M.O., Kuchler, T. et al. Social capital I: measurement and associations with economic mobility. Nature 608, 108–121 (2022). https://doi.org/10.1038/s41586-022-04996-4.\n",
    "\n",
    "* Chetty, R., Jackson, M.O., Kuchler, T. et al. Social capital II: determinants of economic connectedness. Nature 608, 122–134 (2022). https://doi.org/10.1038/s41586-022-04997-3.\n",
    "\n",
    "------------\n",
    "Important Terminology for this notebook:\n",
    "\n",
    "SES: socioeconomic status\n",
    "\n",
    "Economic Connectedness(EC): The share of high-SES friends among individuals with low-SES. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the list of all the unique files used in this notebook:\n",
    "\n",
    "For the **first** plot\n",
    "* 'social_capital_county.csv'\n",
    "* 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'\n",
    "\n",
    "For the **second** plot\n",
    "* 'county_outcomes_simple.csv'\n",
    "\n",
    "For the **third** plot\n",
    "* 'zip_covariates.csv', converted file from original 'zip_covariates.dta'\n",
    "* 'social_capital_zip.csv'\n",
    "\n",
    "For the **fourth** plot\n",
    "* 'social_capital_high_school.csv'\n",
    "\n",
    "For the **fifth** plot\n",
    "* 'college_characteristics.csv', converted file from original 'college_characteristics.dta'\n",
    "* 'social_capital_college.csv'\n",
    "\n",
    "-\n",
    "In some plots files from previous ones were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries we will use throughout this notebook.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Geography of Social Capital in the United States\n",
    "This is a replication of Figure 2a of the first paper, making it look like the interactive one we can find available online at https://www.socialcapital.org/.\n",
    "\n",
    "The map is constructed using Plotly. The data are displayed per county. When the point hovers each county, it  displays the name of the county, the state it belongs to, the FIPS code of the county and the economic connectedness of the county. If there are no data for a particular county, it is painted with a distinct color (gold) and the economic connectedness is given as \"NA\".\n",
    "\n",
    "Data for social capital can be found at the [Social Capital Atlas Datasets](https://data.humdata.org/dataset/social-capital-atlas).\n",
    "\n",
    "\n",
    "The share of high-SES friends among individuals with low SES — which we term economic connectedness — is among the strongest predictors of upward income mobility identified to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the csv file named 'Social Capital Atlas - US Counties.csv', from the [Social Capital Atlas Datasets](https://data.humdata.org/dataset/social-capital-atlas) website. \n",
    "\n",
    "After reading the relevant READme file, we will use 'Social Capital Atlas - US Counties.csv', because it contains the Economic Connectedness per county (column: ec_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file and create dataframe named sc_county.\n",
    "sc_county = pd.read_csv('social_capital_county.csv')\n",
    "\n",
    "#Show the first 5 rows\n",
    "sc_county.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the documentation of the csv file and we find that the column 'county' represents a 5-digit code for every county in USA. Unfortunately in this dataset, if a county-code starts with '0', it is ommited. So we will make every county code 5-digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit the column 'county' as string.\n",
    "sc_county[\"county\"] = sc_county[\"county\"].astype('string')\n",
    "\n",
    "#Make the ec_county column float and round it to two decimals.\n",
    "sc_county['ec_county'] = sc_county['ec_county'].astype('float')\n",
    "sc_county['ec_county'] = sc_county['ec_county'].round(2)\n",
    "\n",
    "#Use the zfill method to make every county-code 5-digit.\n",
    "sc_county[\"county\"] = sc_county[\"county\"].str.zfill(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create quantiles based on Economic Connectedness.\n",
    "sc_county['quantile'] = pd.qcut(sc_county['ec_county'], q=10, precision=3)\n",
    "\n",
    "#Create a list containing the labels we want to use in our graph later.\n",
    "label_list = ['<0.58','0.58-0.67','0.67-0.72','0.72-0.76', \n",
    "              '0.76-0.81', '0.81-0.85','0.85-0.90',\n",
    "              '0.90-0.97','0.97-1.06','>1.06']\n",
    "\n",
    "#Create a new column matching the quantile a county is in, with the proper label.\n",
    "sc_county['quantile_labels'] = pd.qcut(sc_county['ec_county'], q=10, labels = label_list)\n",
    "\n",
    "#Make columns 'quantile_labels','quantile' and 'ec_county' strings for better manipulation.\n",
    "sc_county['quantile_labels'] = sc_county['quantile_labels'].astype('string')\n",
    "sc_county['quantile'] = sc_county['quantile'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the NaN values with the string value 'NA'.\n",
    "sc_county['quantile_labels'] = sc_county['quantile_labels'].fillna('NA')\n",
    "\n",
    "#Create a new column with ec_county value as string for better manipulation in visualization\n",
    "sc_county['ec_county_str'] = sc_county['ec_county']\n",
    "sc_county['ec_county_str'] = sc_county['ec_county_str'].astype('string')\n",
    "sc_county['ec_county_str'] = sc_county['ec_county_str'].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from this github repository the counties and their FIPS codes.\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "    \n",
    "#Create a custom color list starting from red as the lowest quantile of EC and blue the highest.\n",
    "color_list = {'<0.58': '#67001f',\n",
    "              '0.58-0.67': '#b6202f',\n",
    "              '0.67-0.72': '#dd6f59',\n",
    "              '0.72-0.76':  '#f7b799',\n",
    "              '0.76-0.81': '#fae7dc',\n",
    "              '0.81-0.85': '#e2edf3',\n",
    "              '0.85-0.90':  '#a7d0e4',\n",
    "              '0.90-0.97': '#559ec9',\n",
    "              '0.97-1.06': '#256baf',\n",
    "              '>1.06' : '#053061',\n",
    "              'NA':'#FFD700'}\n",
    "\n",
    "#Create a list for category order.\n",
    "category_list = ['>1.06', '0.97-1.06','0.90-0.97','0.85-0.90', \n",
    "                 '0.81-0.85', '0.76-0.81','0.72-0.76', \n",
    "                 '0.67-0.72', '0.58-0.67','<0.58']\n",
    "  \n",
    "#Order the column based on the category_list. \n",
    "category_order_dict = {'quantile_labels': category_list}  \n",
    "\n",
    "#Data we want to display when map value is hovered.\n",
    "hover_data_dict = {\n",
    "        'county': True,\n",
    "        'ec_county_str': True,\n",
    "        'quantile_labels': False\n",
    "        }\n",
    "    \n",
    "#Create the plot.    \n",
    "fig = px.choropleth(sc_county, geojson=counties, \n",
    "                           locations='county', \n",
    "                           color='quantile_labels',\n",
    "                           color_discrete_map = color_list,\n",
    "                           category_orders = category_order_dict,\n",
    "                           scope=\"usa\",\n",
    "                           hover_name = sc_county['county_name'],\n",
    "                           hover_data = hover_data_dict,\n",
    "                           labels = {'ec_county_str' : 'Economic Connectedness',\n",
    "                                     'quantile_labels':'Economic connectedness'}\n",
    "                   )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Economic Connectivity and Outcomes\n",
    "\n",
    "This is a replication on the [Figure 4](https://www.nature.com/articles/s41586-022-04996-4/figures/4) of the first paper. The figure is a scatter plot of upward income mobility against economic connectedness (EC) for the 200 most populous US counties. The income mobility is obtained from the [Opportunity Atlas](https://www.nber.org/papers/w25147), whose replication data can be found [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NKCQM1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the 200 most populous US counties.\n",
    "sc_county['pop2018'] = sc_county['pop2018'].astype(float)\n",
    "sc_county_n200 = sc_county.nlargest(200, ['pop2018'])\n",
    "\n",
    "#Make a string column named 'full_county' to merge later the dataframes.\n",
    "sc_county_n200[\"full_county\"] = sc_county_n200[\"county\"].astype('str') \n",
    "\n",
    "#Print the 5 most populous US counties.\n",
    "sc_county_n200.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the csv file named 'county_outcomes.csv', from the [Replication Data for: The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NKCQM1) website. It contains the predicted household income rank in adulthood for children in the 1978–1983 birth cohorts with parents at the 25th percentile of the national income distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file and create the county_outcomes df.\n",
    "county_outcomes = pd.read_csv('county_outcomes_simple.csv')\n",
    "\n",
    "#In the sc_county df we have a 5-digit code for every county.\n",
    "#In the county_outcomes df we have a 2-digit code for the state and a 3-digit code for the county.\n",
    "\n",
    "#Make the county column string type.\n",
    "county_outcomes[\"county\"] = county_outcomes[\"county\"].apply(str)\n",
    "\n",
    "#Make all the county codes 3-digit, because 0 in the beggining is ommited.\n",
    "county_outcomes[\"county\"] = county_outcomes[\"county\"].str.zfill(3)\n",
    "\n",
    "#Same for the states.\n",
    "county_outcomes[\"state\"] = county_outcomes[\"state\"].apply(str)\n",
    "county_outcomes[\"state\"] = county_outcomes[\"state\"].str.zfill(2)\n",
    "\n",
    "#Merge the 2-digit code of the state with the 3-digit code of the county to have the same 5-digit code of the\n",
    "#sc_county df. Create a new column named 'full_county' with the 5-digit codes.\n",
    "county_outcomes['full_county'] = county_outcomes[\"state\"] + county_outcomes[\"county\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new copy of the 'county_outcomes' df, named 'upward_mobility', to only get the columns we need.\n",
    "\n",
    "We need the county code 'full_county' we created, to merge the df of the EC with the one with the upward mobility ranking.\n",
    "\n",
    "Based on the documentation (CodeBook-for-Table-2.pdf) we downloaded from the same website,\n",
    "we want the column ('kfr_pooled_pooled_p25') with the upward mobility ranking of all the races (pooled) and all the genders (pooled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the df with the county code and the upward mobility ranking.\n",
    "upward_mobility = county_outcomes[['full_county','kfr_pooled_pooled_p25']].copy()\n",
    "\n",
    "#Multiply by 100 to get the percentage.\n",
    "upward_mobility['Rank_percentage'] = upward_mobility['kfr_pooled_pooled_p25'] * 100\n",
    "\n",
    "upward_mobility[\"full_county\"] = upward_mobility[\"full_county\"].apply(str)\n",
    "\n",
    "#Print the first 5 rows.\n",
    "upward_mobility.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We merge the df with the EC and the df with the upward mobility.\n",
    "scatterfinal = pd.merge(sc_county_n200, upward_mobility,on='full_county',how='left')\n",
    "\n",
    "scatterfinal[\"county_name\"] = scatterfinal[\"county_name\"].astype('string')\n",
    "\n",
    "scatterfinal = scatterfinal[['county', 'county_name', 'ec_county', 'kfr_pooled_pooled_p25', 'Rank_percentage']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the EC and the upward mobility of 5 counties we want to visualize in our scatterplot\n",
    "scatterfinal.loc[scatterfinal['county_name'] == 'San Francisco, California']\n",
    "scatterfinal.loc[scatterfinal['county_name'] == 'New York, New York']\n",
    "scatterfinal.loc[scatterfinal['county_name'] == 'Salt Lake, Utah']\n",
    "\n",
    "#Indianapolis official county name is Marion\n",
    "scatterfinal.loc[scatterfinal['county_name'] == 'Marion, Indiana'] \n",
    "\n",
    "#Minneapolis official county name is Hennepin\n",
    "scatterfinal.loc[scatterfinal['county_name'] == 'Hennepin, Minnesota'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick google search we found out that Indianapolis is on Marion, Indiana county and Minneapolis is on Hennepin, Minnesota county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set style to darkgrid\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "#Make the scatterplot using lmplot\n",
    "snsfig = sns.lmplot(x='ec_county', y='kfr_pooled_pooled_p25', data=scatterfinal, aspect=2)\n",
    "\n",
    "#Set labels on axis\n",
    "snsfig.set(\n",
    "    xlabel = 'Economic Connectedness', \n",
    "    ylabel=\"\"\"    Predicted Household Income Rank\n",
    "              For Children With Parents at 25th Percintile\"\"\")\n",
    "\n",
    "#Annotate San Fransisco\n",
    "snsfig.ax.annotate('San Fransisco',\n",
    "                xy = (1.31,0.5),\n",
    "                xytext=(1.31,0.375),\n",
    "                xycoords='data',  \n",
    "                arrowprops=dict(facecolor='black',width=2,headwidth=10.0,headlength=10.0,shrink=0))\n",
    "\n",
    "#Annotate New York City\n",
    "snsfig.ax.annotate('New York City',\n",
    "                xy = (0.827,0.42),\n",
    "                xytext=(0.827,0.5),\n",
    "                xycoords='data',  \n",
    "                arrowprops=dict(facecolor='black',width=2,headwidth=10.0,headlength=10.0,shrink=0))\n",
    "\n",
    "#Annotate Salt Lake City\n",
    "snsfig.ax.annotate('Salt Lake City',\n",
    "                xy = (0.965,0.455),\n",
    "                xytext=(0.965,0.51),\n",
    "                xycoords='data',  \n",
    "                arrowprops=dict(facecolor='black',width=2,headwidth=10.0,headlength=10.0,shrink=0))\n",
    "\n",
    "#Annotate Indianapolis\n",
    "snsfig.ax.annotate('Indianapolis',\n",
    "                xy = (0.642,0.345),\n",
    "                xytext=(0.7,0.33),\n",
    "                xycoords='data',  \n",
    "                arrowprops=dict(facecolor='black',width=2,headwidth=10.0,headlength=10.0,shrink=0))\n",
    "\n",
    "#Annotate Minneapolis\n",
    "snsfig.ax.annotate('Minneapolis',\n",
    "                xy = (0.975,0.427),\n",
    "                xytext=(1.05,0.375),\n",
    "                xycoords='data',  \n",
    "                arrowprops=dict(facecolor='black',width=2,headwidth=10.0,headlength=10.0,shrink=0))\n",
    "\n",
    "\n",
    "#Set xlim\n",
    "snsfig.set(xlim=(0.4,1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Upward Income Mobility, Economic Connectedness, and Median House Income\n",
    "\n",
    "This is a replication of the [Figure 6 of the first paper](https://www.nature.com/articles/s41586-022-04996-4/figures/6). The figure is a scatter plot of economic connectedness (EC) against median household income. We combine data from replication package of the papers (ACS and Opportunity Atlas) with data from the Social Capital Atlas Datasets. The color of the dots corresponds to the child's income rank in adulthood given that the parents' income is in the 25th percentile. The colors correspond to five intervals, which are the quintiles dividing our data. We need the compiled csv file 'zip_covariates.csv' to get the median household income and the upward mobility for each zip code. We got the file from the public data of the replication package, converting it to csv for better manipulation in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert zip_covariates.dta to csv file\n",
    "data = pd.io.stata.read_stata('zip_covariates.dta')\n",
    "data.to_csv('zip_covariates.csv')\n",
    "\n",
    "\n",
    "#Use the zip_covariates.csv file which contains upward mobility and median household income per zip code.\n",
    "upward_mobility = pd.read_csv('zip_covariates.csv', index_col=0)\n",
    "\n",
    "#Use the bin edges we want for our scatterplot.\n",
    "bins=[0, 0.38 ,0.41, 0.44, 0.48, 1]\n",
    "\n",
    "#Create new column named 'quantile' to divide our data into 5 bins.\n",
    "upward_mobility['quantile'] = pd.cut(upward_mobility['kfr_pooled_pooled_p25'], bins = bins)\n",
    "\n",
    "#Print 5 first rows.\n",
    "upward_mobility.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the columns we need.\n",
    "upward_mobility = upward_mobility[['zip', 'kfr_pooled_pooled_p25', 'med_inc_2018', 'pop2018', 'quantile']]\n",
    "\n",
    "#Print the df.\n",
    "upward_mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv containing the Economic Connectedness per zip code from Social Capital Atlas Datasets.\n",
    "social_capital_zip = pd.read_csv('social_capital_zip.csv')\n",
    "\n",
    "#Keep only the rows we need.\n",
    "social_capital_zip =  social_capital_zip[['zip', 'ec_zip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two dataframes .\n",
    "ec_vs_um = pd.merge(social_capital_zip, upward_mobility, on = 'zip', how = 'left')\n",
    "\n",
    "#Print the merged dataframe.\n",
    "ec_vs_um.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows containing NaN values from the columns we need information to be non NaN.\n",
    "ec_vs_um = ec_vs_um[ec_vs_um['ec_zip'].notna()]\n",
    "ec_vs_um = ec_vs_um[ec_vs_um['med_inc_2018'].notna()]\n",
    "ec_vs_um = ec_vs_um[ec_vs_um['kfr_pooled_pooled_p25'].notna()]\n",
    "ec_vs_um = ec_vs_um[ec_vs_um['quantile'].notna()]\n",
    "\n",
    "ec_vs_um.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the zip codes where median household income is greater than 30.000$ and lower than 100.000$.\n",
    "ec_vs_um  = ec_vs_um[ec_vs_um['med_inc_2018'] >= 30000]\n",
    "ec_vs_um  = ec_vs_um[ec_vs_um['med_inc_2018'] <= 100000]\n",
    "\n",
    "#Drop zip codes with population that is included in the 10th percentile of the whole dataset. The least populated.\n",
    "ec_vs_um['pop_deciles'] = pd.qcut(ec_vs_um['pop2018'], 10, labels = False)\n",
    "ec_vs_um  =  ec_vs_um[ec_vs_um['pop_deciles'] != 0]\n",
    "\n",
    "#Convert now quantile column to string for label manipulation.\n",
    "ec_vs_um['quantile'] = ec_vs_um['quantile'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our labels for the scatterplot.\n",
    "q_labels = ['> 48','44 - 48','41 - 44','38 - 41','< 38']\n",
    "\n",
    "#Create a function that converts the quantile span to a specific label.\n",
    "def compute_label(df):    \n",
    "    if (df['quantile'] == '(0.0, 0.38]'):\n",
    "        return q_labels[4]\n",
    "    elif (df['quantile'] == '(0.38, 0.41]'):\n",
    "        return q_labels[3]\n",
    "    elif (df['quantile'] == '(0.41, 0.44]'):\n",
    "        return q_labels[2]\n",
    "    elif (df['quantile'] == '(0.44, 0.48]'):\n",
    "        return q_labels[1]\n",
    "    elif (df['quantile'] == '(0.48, 1.0]'):\n",
    "        return q_labels[0]\n",
    "\n",
    "#Apply the fucntion and create a new column named 'Upward Mobility'.    \n",
    "ec_vs_um['Upward Mobility'] = ec_vs_um.apply(compute_label, axis = 1)   \n",
    "\n",
    "#Print the df.\n",
    "ec_vs_um.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create our scatterplot using seaborn. \n",
    "\n",
    "# Make the scatterplot with better analysis\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "#X-axis has median household income per zip codes.\n",
    "#y-axis Economic Connectedness per zip codes.\n",
    "\n",
    "#Hue is the quantile that each zip code is included based on upward moblity.\n",
    "\n",
    "#Create custom color pallete from red to blue.\n",
    "colors = [\"#191970\",\"#009acd\",\"#F2D2BD\",\"#FFA500\",\"#cd0000\"]\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.scatterplot(data = ec_vs_um, x='med_inc_2018', y='ec_zip', \n",
    "                     hue = 'Upward Mobility',palette = colors, hue_order = q_labels, alpha=0.75)\n",
    "\n",
    "ax.set_xticks([30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000])\n",
    "\n",
    "#Set label names.\n",
    "ax.set(xlabel = 'Median Household Income in ZIP codes (US$)',\n",
    "       ylabel = 'Economic Connectedness')\n",
    "\n",
    "#Legend attributes.\n",
    "plt.legend(loc='lower right', fontsize=5)\n",
    "\n",
    "plt.rcParams[\"figure.autolayout\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Friending Bias and Exposure by High School\n",
    "\n",
    "This is a replication of [Figure 5a of the second paper](https://www.nature.com/articles/s41586-022-04997-3/figures/5). The figure depicts the Socio-Economic Status (SES) of parents against the friending bias of students of low SES, with data from the Social Capital Atlas Datasets. \n",
    "\n",
    "Both $x$ and $y$ axis are percentages and the $y$ axis are reversed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the documentation, the columns we need from the csv file 'social_capital_high_school' to create the plot are:\n",
    "\n",
    "* 'high_school': the code for each high school\n",
    "\n",
    "* 'high_school_name': the name for each high school\n",
    "\n",
    "* 'exposure_parent_ses_hs': Mean exposure to high-parental-SES individuals by high school for\n",
    "low-parental-SES individuals:\n",
    "\n",
    "* 'bias_parent_ses_hs': Economic connectedness with parental SES divided by Mean exposure to high-parental-SES individuals by high school for low-parental-SES individuals, all subtracted from one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the csv file named 'social_capital_high_school.csv' which contains the information we need and create new df.\n",
    "sc_high_school = pd.read_csv('social_capital_high_school.csv')\n",
    "\n",
    "#Create a new column to calculate Share of high-parental-SES students %.\n",
    "sc_high_school['parent_exposure'] =  sc_high_school['exposure_parent_ses_hs']\n",
    "\n",
    "#Create a new column to calculate Friending Bias among low parental-SES students %.\n",
    "sc_high_school['parent_bias'] =  sc_high_school['bias_parent_ses_hs']\n",
    "\n",
    "#Keep only the not NaN values of the columns we created.\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_exposure'].notna()]\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_bias'].notna()]\n",
    "\n",
    "#Make the proper calculations as per description.\n",
    "sc_high_school['parent_exposure'] = (sc_high_school['parent_exposure'] / 2) * 100  \n",
    "sc_high_school['parent_bias'] =  sc_high_school['parent_bias'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the data with the high schools we want to show on the scatterplot.\n",
    "high_school_codes = [\"00941729\",\"060474000432\",\"170993000942\",\"170993001185\",\n",
    "                     \"170993003989\",\"171449001804\",\"250327000436\",\"360009101928\",\n",
    "                     \"370297001285\",\"483702004138\",\"250843001336\",\"062271003230\",\n",
    "                     \"010237000962\",\"00846981\",\"00852124\"]\n",
    "\n",
    "# Create a dataframe with the data above.\n",
    "high_schools_to_show = pd.DataFrame(high_school_codes, columns=['high_school'])\n",
    "  \n",
    "#Merge the new df with the social_high_school to get all the information for the high schools we want.\n",
    "high_schools_to_show =  pd.merge(high_schools_to_show, sc_high_school,on=['high_school'],how='left')\n",
    "\n",
    "#Keep only the columns we need.\n",
    "high_schools_to_show = high_schools_to_show[['high_school','high_school_name','parent_exposure','parent_bias']].copy()\n",
    "\n",
    "#Print the df.\n",
    "high_schools_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only the values based on the limits we will set later, to avoid ggplot removing automatically them and showing us a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x-axis, 0 to 90 are the x-axis limits we will set.\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_exposure'] >= 0]\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_exposure'] <= 90]\n",
    "\n",
    "#for y-axis, -15 to 30 are the y-axis limits we will set.\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_bias'] >= -15]\n",
    "sc_high_school = sc_high_school[sc_high_school['parent_bias'] <= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Jitter to the high school name labels so they don't overlap on the plot.\n",
    "\n",
    "#the numbers are approximates for a more clear scatterplot.\n",
    "\n",
    "high_schools_jitter = high_schools_to_show.copy()\n",
    "\n",
    "#Dalton School\n",
    "high_schools_jitter.at[0,'parent_exposure'] = 67\n",
    "\n",
    "#Berkeley HS\n",
    "high_schools_jitter.at[1,'parent_bias'] = 15\n",
    "\n",
    "#Lane Technical HS\n",
    "high_schools_jitter.at[2,'parent_exposure'] = 53\n",
    "high_schools_jitter.at[2,'parent_bias'] = -4\n",
    "\n",
    "#Lincoln Park HS\n",
    "high_schools_jitter.at[3,'parent_exposure'] = 44\n",
    "\n",
    "#Payton College Preparatory HS\n",
    "high_schools_jitter.at[4,'parent_bias'] = 4\n",
    "high_schools_jitter.at[4,'parent_exposure'] = 64\n",
    "\n",
    "#Evanston Twp HS\n",
    "high_schools_jitter.at[5,'parent_exposure'] = 70\n",
    "\n",
    "#Cambridge Rindge And Latin\n",
    "high_schools_jitter.at[6,'parent_bias'] = 9\n",
    "\n",
    "#Brooklyn Technical HS\n",
    "high_schools_jitter.at[7,'parent_bias'] = -2\n",
    "\n",
    "#West Charlotte HS\n",
    "high_schools_jitter.at[8,'parent_exposure'] = 14\n",
    "\n",
    "#New Bedford HS\n",
    "high_schools_jitter.at[10,'parent_bias'] = 3\n",
    "\n",
    "#John L Leflore Magnet School\n",
    "high_schools_jitter.at[12,'parent_bias'] = -1.5\n",
    "\n",
    "#Bishop Gorman HS\n",
    "high_schools_jitter.at[13,'parent_bias'] = 5\n",
    "high_schools_jitter.at[13,'parent_exposure'] = 86\n",
    "\n",
    "#Phillips Exeter Academy\n",
    "high_schools_jitter.at[14,'parent_exposure'] = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import plotnine to create a ggplot style scatterplot.\n",
    "import plotnine as p9\n",
    "\n",
    "#Make the plot.\n",
    "(p9.ggplot(data=sc_high_school, mapping=p9.aes(x='parent_exposure', y='parent_bias')) \n",
    " \n",
    " + p9.geom_point(alpha = 0.12)\n",
    " \n",
    " + p9.scales.scale_y_continuous(limits = (30, -15), breaks = (30, 20, 10, 0, -10), trans = \"reverse\")\n",
    " \n",
    " + p9.scales.scale_x_continuous(limits = (0, 90), breaks = (0, 20, 40, 60, 80, 100)) \n",
    " \n",
    " + p9.labels.labs(x = \"Share of high-parental-SES students (%)\", \n",
    "                  y = \"Friending bias among low-parental-SES students (%)\") \n",
    " \n",
    " + p9.themes.theme(axis_text_x = p9.element_text(size = 8),\n",
    "                   axis_text_y = p9.element_text(size = 8),\n",
    "                   axis_title = p9.element_text(size = 8))\n",
    " \n",
    " + p9.geom_point(data = high_schools_to_show,color = \"aquamarine\", size = 3, alpha = 0.8)\n",
    " \n",
    " + p9.geom_label(data = high_schools_jitter, color = \"black\",\n",
    "                        alpha = 0.9, label_size = 0.1, label_padding = 0.5, \n",
    "                        mapping = p9.aes(label = 'high_school_name'), size = 5.7, \n",
    "                        position= p9.position_jitter(width= 3,height=5), nudge_x= -2,nudge_y=2)\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Friending Bias vs. Racial Diversity\n",
    "\n",
    "This is a replication of [Extended Data Figure 3](https://www.nature.com/articles/s41586-022-04997-3/figures/9) of the second paper. The figure depicts friending bias against racial diversity. Racial diversity is defined by the [Herfindahl-Hirschman Index (HHI)](https://en.wikipedia.org/wiki/Herfindahl%E2%80%93Hirschman_index), borrowed from investing. Translated here, it is $ 1−\\sum_{i}{s_i}^2$, where $s_i$ is the fraction of race/ethnicity $i$ (Black, White, Asian, Hispanic, Native American).\n",
    "\n",
    "As you can see, the figure contains two scatter plots with their respective regression lines, one for college data and the other for neighborhood data. Each of the two plots displays binned data (that's why you don't see loads of dots and diamonds). The bins are produced by dividing the $x$-axis into ventiles (i.e., 5 percentile point bins); then we plot the mean of the $y$-axis variable against the appropriate mean of the $x$-axis variable in each ventile. \n",
    "\n",
    "The mean of the $x$-axis variable, the HHI index, is the weighted mean of HHI:\n",
    "\n",
    "* For the college plot, the weights are given by the mean number of students per cohort.\n",
    "\n",
    "* For the neighborhood plot, the weights are given by the number of children with below-national-median parental household income.\n",
    "\n",
    "The $y$-axis variable:\n",
    "\n",
    "* For the college plot, it is the mean of the college friending bias.\n",
    "\n",
    "* For the neighborhood plot, it is the mean of the neighborhood friending bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Neighborhood plot\n",
    "\n",
    "We will make the proper data manipulation to create the data for the Neighborhood plot (Orange line).\n",
    "\n",
    "As we need the fractions of race/ethnicity, we will use again the csv file we converted from the social capital replication package.\n",
    "\n",
    "Spefically we need to use the columns below:\n",
    "* 'share_white_2018' for the fraction of the race 'white'\n",
    "* 'share_black_2018' for the fraction of the race 'black'\n",
    "* 'share_natam_2018' for the fraction of the ethnicity 'native American'\n",
    "* 'share_asian_2018' for the fraction of the ethnicity 'asian'\n",
    "* 'share_hispanic_2018' for the fraction of the ethnicity 'hispanic'\n",
    "\n",
    "To calculate the HHI per zip code.\n",
    "\n",
    "We also need the column 'num_below_p50', so we can calculate the appropriate weighted mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "racial_diversity = pd.read_csv('zip_covariates.csv', index_col=0)\n",
    "\n",
    "#Print first 5 rows\n",
    "racial_diversity.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need again the csv file named 'social_capital_zip.csv' because we ant to use the column:\n",
    "\n",
    "* 'nbhd_bias_zip', containing the neighborhood friending bias per zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "sc_zip = pd.read_csv('social_capital_zip.csv')\n",
    "\n",
    "#Print first 5 rows\n",
    "sc_zip.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two Dataframes, named friending bias vs racial diversity\n",
    "fb_vs_rd = pd.merge(sc_zip, racial_diversity, on = 'zip', how = 'left')\n",
    "\n",
    "#Print first 5 rows\n",
    "fb_vs_rd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the hhi based on the mathematical formula on the description\n",
    "hhi = 1 - ((fb_vs_rd['share_white_2018'] ** 2) +  \n",
    "           (fb_vs_rd['share_black_2018'] ** 2) + \n",
    "           (fb_vs_rd['share_natam_2018'] ** 2) +\n",
    "           (fb_vs_rd['share_asian_2018'] ** 2) +\n",
    "           (fb_vs_rd['share_hawaii_2018'] ** 2) +\n",
    "           (fb_vs_rd['share_hispanic_2018'] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column with the hhi per zip code\n",
    "fb_vs_rd['Herfindahl-Hirschman-Index'] = hhi\n",
    "\n",
    "#Keep the columns we need in a copy of the dataset\n",
    "fb_vs_rd = fb_vs_rd[['zip','nbhd_bias_zip','Herfindahl-Hirschman-Index','num_below_p50_x']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale neighborhood friending bias to 100\n",
    "fb_vs_rd['nbhd_bias_zip'] = fb_vs_rd['nbhd_bias_zip'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column with 20 bins, ventiles, based on the HHI.\n",
    "fb_vs_rd['bin'] = pd.qcut(fb_vs_rd['Herfindahl-Hirschman-Index'], 20, labels = False)\n",
    "\n",
    "#Print first 5 rows\n",
    "fb_vs_rd.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reached the pre-final phase. \n",
    "\n",
    "Basically we need to create a dataframe with 20 points for the neighborhood plot.\n",
    "\n",
    "Both the plots are the mean of the  𝑦 -axis variable against the appropriate mean of the  𝑥 -axis variable in each ventile as per description.\n",
    "\n",
    "* In this case the y-axis variable, is the mean of the neighborhood friending bias.\n",
    "\n",
    "\n",
    "* For the x-axis, it is the weighted mean of the HHI. The weights are given by the number of children with below-national-median parental household income ('num_below_p50' column).\n",
    "\n",
    "So we need to calculate both the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the weighted mean of the HHI.\n",
    "\n",
    "#Group the data by bin, aka ventile.\n",
    "grouped_x = fb_vs_rd.groupby('bin')\n",
    "\n",
    "#Create a function to calculate the weighted mean.\n",
    "def wavg(group):\n",
    "    d = group['Herfindahl-Hirschman-Index']\n",
    "    w = group['num_below_p50_x']\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "#Apply the function to the grouped data.\n",
    "nei_x = grouped_x.apply(wavg)\n",
    "\n",
    "#Calculate the mean of the neighborhood friending bias.\n",
    "\n",
    "#Group the data by bin, aka ventile.\n",
    "grouped_y = fb_vs_rd.groupby('bin')\n",
    "\n",
    "#Create a function to calculate the mean.\n",
    "def binmean(group):\n",
    "    d = group['nbhd_bias_zip']\n",
    "    return d.sum() / d.count()\n",
    "\n",
    "#Apply the function to the grouped data.\n",
    "nei_y = grouped_y.apply(binmean)\n",
    "\n",
    "#Create a dataframe with the x-axis points.\n",
    "nei_df_x = pd.DataFrame (nei_x, columns = ['Point'])\n",
    "nei_df_x = nei_df_x.reset_index(level=0)\n",
    "\n",
    "#Create a dataframe with the y-axis points.\n",
    "nei_df_y = pd.DataFrame (nei_y, columns = ['Point'])\n",
    "nei_df_y = nei_df_y.reset_index(level=0)\n",
    "\n",
    "#Merge the two dataframes.\n",
    "nei_df = pd.merge(nei_df_x, nei_df_y, on = 'bin', how = 'inner')\n",
    "\n",
    "#Create a column named type, so we can use it later to plot based on hue.\n",
    "nei_df['Type'] = 'Neighborhood'\n",
    "\n",
    "#Print the dataframe\n",
    "nei_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'nei_df', contains all the points we need to plot the neighborhood plot. We will merge it later with the counterpart of the college plot to create our final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 College plot\n",
    "\n",
    "We will make the proper data manipulation to create the data for the College plot (Blue line).\n",
    "\n",
    "As we need the fractions of race/ethnicity, we will use the converted .dta file, to csv, named 'college_characteristics.csv' from the social capital replication package.\n",
    "\n",
    "Spefically we need to use the columns below:\n",
    "* 'black_share_fall_2000' for the fraction of the race 'black'\n",
    "* 'asian_or_pacific_share_fall_2000' for the fraction of the race 'asian'\n",
    "* 'hisp_share_fall_2000' for the fraction of the ethnicity 'hispanic'\n",
    "\n",
    "To calculate the HHI per college.\n",
    "\n",
    "We notice that there is no column for the white race. So we have to calculate it.\n",
    "\n",
    "We also need the column ''mean_students_per_cohort', so we can calculate the appropriate weighted mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert college_characteristics.dta to csv file\n",
    "data = pd.io.stata.read_stata('college_characteristics.dta')\n",
    "data.to_csv('college_characteristics.csv')\n",
    "\n",
    "\n",
    "#Read the file\n",
    "college_characteristics = pd.read_csv('college_characteristics.csv', index_col=0)\n",
    "\n",
    "#Print first 5 rows\n",
    "college_characteristics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the file 'social_capital_college.csv' containing the friending bias per college. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "sc_college = pd.read_csv('social_capital_college.csv')\n",
    "\n",
    "#Print first 5 rows\n",
    "sc_college.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the two dataframes\n",
    "college_merged = pd.merge(sc_college, college_characteristics, on = 'college', how = 'inner')\n",
    "\n",
    "#Print first 5 rows\n",
    "college_merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column with the fraction for the white race.\n",
    "college_merged['white_share_fall_2000'] =  1 - college_merged['hisp_share_fall_2000'] - college_merged['black_share_fall_2000'] - college_merged['asian_or_pacific_share_fall_2000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the hhi\n",
    "hhi = 1 - ((college_merged['white_share_fall_2000'] ** 2) +  \n",
    "           (college_merged['black_share_fall_2000'] ** 2) + \n",
    "           (college_merged['asian_or_pacific_share_fall_2000'] ** 2) +\n",
    "           (college_merged['hisp_share_fall_2000'] ** 2))\n",
    "\n",
    "#Create a new column with the hhi per college\n",
    "college_merged['Herfindahl-Hirschman-Index'] = hhi\n",
    "\n",
    "#Keep the columns we need in a copy of the df\n",
    "college_merged = college_merged[['zip_x','bias_own_ses_college','Herfindahl-Hirschman-Index','mean_students_per_cohort_y']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale the college friending bias to 100\n",
    "college_merged['bias_own_ses_college'] = college_merged['bias_own_ses_college'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column with 20 bins, ventiles, based on the HHI\n",
    "college_merged['bin'] = pd.qcut(college_merged['Herfindahl-Hirschman-Index'], 20, labels = False)\n",
    "\n",
    "#Print first 5 rows\n",
    "college_merged.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reached the next pre-final phase. \n",
    "\n",
    "Basically we need to create a dataframe with 20 points for the college plot.\n",
    "\n",
    "Both the plots are the mean of the  𝑦 -axis variable against the appropriate mean of the  𝑥 -axis variable in each ventile as per description.\n",
    "\n",
    "* In this case the y-axis variable, it is the mean of the college friending bias.\n",
    "\n",
    "\n",
    "* For the x-axis, it is the weighted mean of the HHI. the weights are given by the mean number of students per cohort.\n",
    "\n",
    "So we need to calculate both the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the weighted mean of the HHI.\n",
    "\n",
    "#Group data per bin, aka ventile\n",
    "grouped_x2 = college_merged.groupby('bin')\n",
    "\n",
    "#Create function that calculates the weighted mean.\n",
    "def wavg(group):\n",
    "    d = group['Herfindahl-Hirschman-Index']\n",
    "    w = group['mean_students_per_cohort_y']\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "#Apply the function to the binned data.\n",
    "coll_x = grouped_x2.apply(wavg)\n",
    "\n",
    "\n",
    "#Calculate the mean of the college friending bias.\n",
    "\n",
    "#Group data per bin, aka ventile\n",
    "grouped_y2 = college_merged.groupby('bin')\n",
    "\n",
    "#Create function that calculates the mean.\n",
    "def binmean(group):\n",
    "    d = group['bias_own_ses_college']\n",
    "    return d.sum() / d.count()\n",
    "\n",
    "#Apply the function to the binned data.\n",
    "coll_y = grouped_y2.apply(binmean)\n",
    "\n",
    "\n",
    "#Create a dataframe with the x-axis points.\n",
    "coll_df_x = pd.DataFrame (coll_x, columns = ['Point'])\n",
    "coll_df_x = coll_df_x.reset_index(level=0)\n",
    "\n",
    "#Create a dataframe with the y-axis points.\n",
    "coll_df_y = pd.DataFrame (coll_y, columns = ['Point'])\n",
    "coll_df_y = coll_df_y.reset_index(level=0)\n",
    "\n",
    "#Merge the two dataframes.\n",
    "coll_df = pd.merge(coll_df_x, coll_df_y, on = 'bin', how = 'inner')\n",
    "\n",
    "#Create a column named type, so we can use it later to plot based on hue.\n",
    "coll_df['Type'] = 'College'\n",
    "\n",
    "#Print the dataframe\n",
    "coll_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'coll_df', contains all the points we need to plot the college plot. Now we will merge it  with the 'neig_df' counterpart to create our final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the nei_df to the coll_df\n",
    "binscatter_df = coll_df.append(nei_df, ignore_index=True)\n",
    "\n",
    "#Print the final df we will use to plot the scatterplot\n",
    "binscatter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot the scatterplot based on hue(Neighborhood or College)\n",
    "\n",
    "#Set style to darkgrid\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "#Make the scatterplot using lmplot\n",
    "binscatter = sns.lmplot(x='Point_x', y='Point_y', hue = 'Type', data = binscatter_df,\n",
    "                        aspect=1.5, markers=[\"D\",\"o\"])\n",
    "\n",
    "binscatter.set(xticks = (0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8))\n",
    "binscatter.set(yticks = (0, 5, 10, 15, 20, 25))\n",
    "\n",
    "#Set labels on axis\n",
    "binscatter.set(xlabel = 'Racial Diversity (Herfindahl-Hirschman Index) in Group', \n",
    "               ylabel = 'Friending Bias among Low-SES Individuals (%)')\n",
    "\n",
    "#Position the legend on the upper left\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#Remove hue legend\n",
    "binscatter._legend.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
